{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2. Creating & Training Audio VAE",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uE6EaH6Q7T2bRlcNoQJIMqR6Jd3-AIeJ",
      "authorship_tag": "ABX9TyMTw/DEeEomzxEjCxJnaquu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratikStar/google-colab/blob/main/2_Creating_%26_Training_Audio_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY05tQrkV_pA"
      },
      "source": [
        "1.  Create VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHIAgyMQE-60"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpskSmkGT4C",
        "outputId": "27a424cd-7bc9-4a31-9332-d2172216c50e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',  force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlnHSb1FDsJ"
      },
      "source": [
        "### Install tensorflow v2.4.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr15BDmekCAy"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlxNWPvvVn1L",
        "outputId": "e63f649a-62aa-4436-eff4-ce93bbaa22f8"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class VAE:\n",
        "    \"\"\"\n",
        "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
        "    with mirrored encoder and decoder components.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,\n",
        "                 conv_strides,\n",
        "                 latent_space_dim):\n",
        "        self.input_shape = input_shape\n",
        "        self.conv_filters = conv_filters\n",
        "        self.conv_kernels = conv_kernels \n",
        "        self.conv_strides = conv_strides \n",
        "        self.latent_space_dim = latent_space_dim \n",
        "        self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.model = None\n",
        "\n",
        "        self._num_conv_layers = len(conv_filters)\n",
        "        self._shape_before_bottleneck = None\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.0001):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss=self._calculate_combined_loss,\n",
        "                           metrics=[self._calculate_reconstruction_loss,\n",
        "                                    self._calculate_kl_loss])\n",
        "\n",
        "    def train(self, x_train, batch_size, num_epochs):\n",
        "        self.model.fit(x_train,\n",
        "                       x_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       shuffle=True)\n",
        "\n",
        "    def save(self, save_folder=\".\"):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "            self.conv_strides,\n",
        "            self.latent_space_dim\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            print(parameters)\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        latent_representations = self.encoder.predict(images)\n",
        "        reconstructed_images = self.decoder.predict(latent_representations)\n",
        "        return reconstructed_images, latent_representations\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, save_folder=\".\"):\n",
        "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        autoencoder = VAE(*parameters)\n",
        "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        autoencoder.load_weights(weights_path)\n",
        "        return autoencoder\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                         + kl_loss\n",
        "        return combined_loss\n",
        "\n",
        "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "        error = y_target - y_predicted\n",
        "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                               K.exp(self.log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "        self._build_autoencoder()\n",
        "\n",
        "    def _build_autoencoder(self):\n",
        "        model_input = self._model_input\n",
        "        model_output = self.decoder(self.encoder(model_input))\n",
        "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        decoder_input = self._add_decoder_input()\n",
        "        dense_layer = self._add_dense_layer(decoder_input)\n",
        "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "    def _add_decoder_input(self):\n",
        "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "    def _add_dense_layer(self, decoder_input):\n",
        "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
        "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "        return dense_layer\n",
        "\n",
        "    def _add_reshape_layer(self, dense_layer):\n",
        "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "    def _add_conv_transpose_layers(self, x):\n",
        "        \"\"\"Add conv transpose blocks.\"\"\"\n",
        "        # loop through all the conv layers in reverse order and stop at the\n",
        "        # first layer\n",
        "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "            x = self._add_conv_transpose_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_transpose_layer(self, layer_index, x):\n",
        "        layer_num = self._num_conv_layers - layer_index\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_decoder_output(self, x):\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=self.conv_kernels[0],\n",
        "            strides=self.conv_strides[0],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "        return output_layer\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        encoder_input = self._add_encoder_input()\n",
        "        conv_layers = self._add_conv_layers(encoder_input)\n",
        "        bottleneck = self._add_bottleneck(conv_layers)\n",
        "        self._model_input = encoder_input\n",
        "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "    def _add_encoder_input(self):\n",
        "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "    def _add_conv_layers(self, encoder_input):\n",
        "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
        "        x = encoder_input\n",
        "        for layer_index in range(self._num_conv_layers):\n",
        "            x = self._add_conv_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_layer(self, layer_index, x):\n",
        "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
        "        conv 2d + ReLU + batch normalization.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        conv_layer = Conv2D(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"encoder_conv_layer_{layer_number}\"\n",
        "        )\n",
        "        x = conv_layer(x)\n",
        "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_bottleneck(self, x):\n",
        "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
        "        layer).\n",
        "        \"\"\"\n",
        "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "        x = Flatten()(x)\n",
        "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "        self.log_variance = Dense(self.latent_space_dim,\n",
        "                                  name=\"log_variance\")(x)\n",
        "\n",
        "        def sample_point_from_normal_distribution(args):\n",
        "            mu, log_variance = args\n",
        "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                      stddev=1.)\n",
        "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "            return sampled_point\n",
        "\n",
        "        x = Lambda(sample_point_from_normal_distribution,\n",
        "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(tf.__version__)\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(32, 64, 64, 64),\n",
        "        conv_kernels=(3, 3, 3, 3),\n",
        "        conv_strides=(1, 2, 2, 1),\n",
        "        latent_space_dim=32\n",
        "    )\n",
        "    autoencoder.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 512, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv2D)   (None, 512, 64, 32)  320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_1 (ReLU)           (None, 512, 64, 32)  0           encoder_conv_layer_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_1 (BatchNormalizatio (None, 512, 64, 32)  128         encoder_relu_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv2D)   (None, 256, 32, 64)  18496       encoder_bn_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_2 (ReLU)           (None, 256, 32, 64)  0           encoder_conv_layer_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_2 (BatchNormalizatio (None, 256, 32, 64)  256         encoder_relu_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv2D)   (None, 128, 16, 64)  36928       encoder_bn_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_3 (ReLU)           (None, 128, 16, 64)  0           encoder_conv_layer_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_3 (BatchNormalizatio (None, 128, 16, 64)  256         encoder_relu_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_4 (Conv2D)   (None, 128, 16, 64)  36928       encoder_bn_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_4 (ReLU)           (None, 128, 16, 64)  0           encoder_conv_layer_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_4 (BatchNormalizatio (None, 128, 16, 64)  256         encoder_relu_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 131072)       0           encoder_bn_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 32)           4194336     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "log_variance (Dense)            (None, 32)           4194336     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 32)           0           mu[0][0]                         \n",
            "                                                                 log_variance[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 8,482,240\n",
            "Trainable params: 8,481,792\n",
            "Non-trainable params: 448\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 32)]              0         \n",
            "_________________________________________________________________\n",
            "decoder_dense (Dense)        (None, 131072)            4325376   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 128, 16, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 128, 16, 64)       36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_1 (ReLU)        (None, 128, 16, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_1 (BatchNormaliza (None, 128, 16, 64)       256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 32, 64)       36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_2 (ReLU)        (None, 256, 32, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_2 (BatchNormaliza (None, 256, 32, 64)       256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 512, 64, 64)       36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_3 (ReLU)        (None, 512, 64, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_3 (BatchNormaliza (None, 512, 64, 64)       256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 512, 64, 1)        577       \n",
            "_________________________________________________________________\n",
            "sigmoid_layer (Activation)   (None, 512, 64, 1)        0         \n",
            "=================================================================\n",
            "Total params: 4,437,505\n",
            "Trainable params: 4,437,121\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 512, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 32)                8482240   \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 512, 64, 1)        4437505   \n",
            "=================================================================\n",
            "Total params: 12,919,745\n",
            "Trainable params: 12,918,913\n",
            "Non-trainable params: 832\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MetCvcrpWJ9g"
      },
      "source": [
        "2. Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4XDb66wU0qt",
        "outputId": "444f838c-6c70-49ef-87b4-0e27152b8836"
      },
      "source": [
        "# from autoencoder import VAE\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 5\n",
        "LATENT_SPACE_DIM=8 # try less than 10?\n",
        "\n",
        "SPECTROGRAMS_PATH = \"/content/drive/MyDrive/Music/VAE-test/spectrogram-01\"\n",
        "FILE_NAME_REGEX = \"^512.*\"\n",
        "\n",
        "def load_fsdd(spectrograms_path):\n",
        "    x_train = []\n",
        "    filenames = []\n",
        "    dataset = {}\n",
        "    for root, _, file_names in os.walk(spectrograms_path):\n",
        "        for file_name in file_names:\n",
        "\n",
        "            if True: # re.match(regex, file_name):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                # print(\"Processing for: \" + file_path)\n",
        "                spectrogram = np.load(file_path) # (n_bins, n_frames, 1) \n",
        "                print(spectrogram.shape)\n",
        "                dataset[file_name] = spectrogram\n",
        "\n",
        "                filenames.append(file_name)\n",
        "                x_train.append(spectrogram)\n",
        "    x_train = np.array(x_train)\n",
        "    x_train = x_train[..., np.newaxis] # -> (number of samples in the dataset, number of bins from stfft, number of frames, 1) = (n, 256, 776, 1)\n",
        "    return x_train, filenames\n",
        "\n",
        "\n",
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(256, 64, 1),\n",
        "        conv_filters=(32, 64, 64, 64), # 16, 8, 8, 4 -> \n",
        "        conv_kernels=(3, 3, 3, 3),\n",
        "        conv_strides=(1, 2, 2, 1),\n",
        "        latent_space_dim=LATENT_SPACE_DIM\n",
        "    )\n",
        "    autoencoder.summary()\n",
        "    autoencoder.compile(learning_rate)\n",
        "    autoencoder.train(x_train, batch_size, epochs)\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x_train, _ = load_fsdd(SPECTROGRAMS_PATH)\n",
        "    autoencoder = train(x_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
        "    model_suffix = \"-\" + str(LATENT_SPACE_DIM) + \"-\" + str(BATCH_SIZE) + \"-\" + str(EPOCHS)\n",
        "    autoencoder.save(\"/content/drive/MyDrive/Music/VAE-test/model\" + model_suffix)\n",
        "    print(\"Model Saved!!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "(256, 64)\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 256, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv2D)   (None, 256, 64, 32)  320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_1 (ReLU)           (None, 256, 64, 32)  0           encoder_conv_layer_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_1 (BatchNormalizatio (None, 256, 64, 32)  128         encoder_relu_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv2D)   (None, 128, 32, 64)  18496       encoder_bn_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_2 (ReLU)           (None, 128, 32, 64)  0           encoder_conv_layer_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_2 (BatchNormalizatio (None, 128, 32, 64)  256         encoder_relu_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv2D)   (None, 64, 16, 64)   36928       encoder_bn_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_3 (ReLU)           (None, 64, 16, 64)   0           encoder_conv_layer_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_3 (BatchNormalizatio (None, 64, 16, 64)   256         encoder_relu_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_4 (Conv2D)   (None, 64, 16, 64)   36928       encoder_bn_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_4 (ReLU)           (None, 64, 16, 64)   0           encoder_conv_layer_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_4 (BatchNormalizatio (None, 64, 16, 64)   256         encoder_relu_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 65536)        0           encoder_bn_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 8)            524296      flatten_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "log_variance (Dense)            (None, 8)            524296      flatten_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 8)            0           mu[0][0]                         \n",
            "                                                                 log_variance[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,142,160\n",
            "Trainable params: 1,141,712\n",
            "Non-trainable params: 448\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "decoder_dense (Dense)        (None, 65536)             589824    \n",
            "_________________________________________________________________\n",
            "reshape_10 (Reshape)         (None, 64, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 64, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_1 (ReLU)        (None, 64, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_1 (BatchNormaliza (None, 64, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 128, 32, 64)       36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_2 (ReLU)        (None, 128, 32, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_2 (BatchNormaliza (None, 128, 32, 64)       256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 64, 64)       36928     \n",
            "_________________________________________________________________\n",
            "decoder_relu_3 (ReLU)        (None, 256, 64, 64)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_3 (BatchNormaliza (None, 256, 64, 64)       256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 64, 1)        577       \n",
            "_________________________________________________________________\n",
            "sigmoid_layer (Activation)   (None, 256, 64, 1)        0         \n",
            "=================================================================\n",
            "Total params: 701,953\n",
            "Trainable params: 701,569\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 256, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 8)                 1142160   \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 256, 64, 1)        701953    \n",
            "=================================================================\n",
            "Total params: 1,844,113\n",
            "Trainable params: 1,843,281\n",
            "Non-trainable params: 832\n",
            "_________________________________________________________________\n",
            "Train on 80 samples\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 2s 24ms/sample - loss: 137272.1602 - _calculate_reconstruction_loss: 0.1361 - _calculate_kl_loss: 1197.1702\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 1s 8ms/sample - loss: 171664.8594 - _calculate_reconstruction_loss: 0.1000 - _calculate_kl_loss: 71677.5078\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 1s 8ms/sample - loss: 106879.0977 - _calculate_reconstruction_loss: 0.1065 - _calculate_kl_loss: 401.8562\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 1s 8ms/sample - loss: 93048.0020 - _calculate_reconstruction_loss: 0.0834 - _calculate_kl_loss: 9636.6084\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 1s 8ms/sample - loss: 56123.4629 - _calculate_reconstruction_loss: 0.0556 - _calculate_kl_loss: 478.3076\n",
            "[(256, 64, 1), (32, 64, 64, 64), (3, 3, 3, 3), (1, 2, 2, 1), 8]\n",
            "Model Saved!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2IhMH3guEs_",
        "outputId": "df548445-d461-4289-edf3-6a10e562f369"
      },
      "source": [
        "# # https://github.com/musikalkemist/generating-sound-with-neural-networks/blob/49d7db32c43d1a04c596cbbb282a9521be1e7fc8/11%20Implementing%20VAE/code/analysis.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "# from autoencoder import VAE\n",
        "# from train import load_mnist\n",
        "\n",
        "\n",
        "def load_fsdd1(spectrograms_path):\n",
        "    dataset = {}\n",
        "    for root, _, file_names in os.walk(spectrograms_path):\n",
        "        for file_name in file_names:\n",
        "\n",
        "            if True: # re.match(regex, file_name):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                # print(\"Processing for: \" + file_path)\n",
        "                spectrogram = np.load(file_path) # (n_bins, n_frames, 1) \n",
        "                dataset[file_name] = spectrogram[..., np.newaxis]\n",
        "    return dataset\n",
        "\n",
        "def select_images(dataset, num_images=10):\n",
        "    sample_keys = np.random.choice(list(dataset.keys()), num_images)\n",
        "    sample_ds = { key: dataset[key] for key in sample_keys }\n",
        "\n",
        "    return sample_ds\n",
        "\n",
        "\n",
        "def plot_reconstructed_images(images, reconstructed_images):\n",
        "    num_images = len(images)\n",
        "    for i, (image, reconstructed_image) in enumerate(zip(images, reconstructed_images)):\n",
        "        # ax = fig.add_subplot(2, num_images, i + 1)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        image = image.squeeze()\n",
        "        img = librosa.display.specshow(image, y_axis='log', x_axis='time', ax=ax)\n",
        "        fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        reconstructed_image = reconstructed_image.squeeze()\n",
        "        recon_img = librosa.display.specshow(reconstructed_image, y_axis='log', x_axis='time', ax=ax)\n",
        "        fig.colorbar(recon_img, ax=ax, format=\"%+2.0f dB\")\n",
        "\n",
        "        # ax.axis(\"off\")\n",
        "        # ax.imshow(image)\n",
        "        # ax = fig.add_subplot(2, num_images, i + num_images + 1)\n",
        "        # ax.axis(\"off\")\n",
        "        # ax.imshow(reconstructed_image)\n",
        "\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_images_encoded_in_latent_space(latent_representations, sample_labels):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(latent_representations[:, 0],\n",
        "                latent_representations[:, 1],\n",
        "                cmap=\"rainbow\",\n",
        "                c=sample_labels,\n",
        "                alpha=0.5,\n",
        "                s=2)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "autoencoder = VAE.load(\"/content/drive/MyDrive/Music/VAE-test/model-8-20-5\")\n",
        "\n",
        "ds = load_fsdd1(\"/content/drive/MyDrive/Music/VAE-test/spectrogram-01/\")\n",
        "\n",
        "\n",
        "num_sample_images_to_show = 80\n",
        "\n",
        "sample_ds = select_images(ds, num_sample_images_to_show)\n",
        "\n",
        "# vals = list(sample_ds.values())\n",
        "# print(vals[0])\n",
        "# print(np.array(vals).shape)\n",
        "\n",
        "reconstructed_images, latent_representations = autoencoder.reconstruct(np.array(list(sample_ds.values())))\n",
        "\n",
        "with open('output.tsv', 'w', newline='') as f_output:\n",
        "    tsv_output = csv.writer(f_output, delimiter='\\t')\n",
        "    for data in latent_representations:\n",
        "        tsv_output.writerow(data)\n",
        "\n",
        "# for i, (image, latent_representation) in enumerate(zip(sample_images, latent_representations)):\n",
        "#     print(str(image) + \" -> \" + str(latent_representation))\n",
        "\n",
        "# plot_reconstructed_images(sample_images, reconstructed_images)\n",
        "\n",
        "# num_images = 6000\n",
        "# sample_images, sample_labels = select_images(x_test, y_test, num_images)\n",
        "# _, latent_representations = autoencoder.reconstruct(sample_images)\n",
        "# plot_images_encoded_in_latent_space(latent_representations, sample_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2. Creating & Training Audio VAE",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uE6EaH6Q7T2bRlcNoQJIMqR6Jd3-AIeJ",
      "authorship_tag": "ABX9TyMR0HP5jMwCSK5rzT45SHG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratikStar/google-colab/blob/main/2_Creating_%26_Training_Audio_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY05tQrkV_pA"
      },
      "source": [
        "1.  Create VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHIAgyMQE-60"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpskSmkGT4C",
        "outputId": "27a424cd-7bc9-4a31-9332-d2172216c50e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',  force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlnHSb1FDsJ"
      },
      "source": [
        "### Install tensorflow v2.4.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr15BDmekCAy"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlxNWPvvVn1L"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class VAE:\n",
        "    \"\"\"\n",
        "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
        "    with mirrored encoder and decoder components.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,\n",
        "                 conv_strides,\n",
        "                 latent_space_dim):\n",
        "        self.input_shape = input_shape\n",
        "        self.conv_filters = conv_filters\n",
        "        self.conv_kernels = conv_kernels \n",
        "        self.conv_strides = conv_strides \n",
        "        self.latent_space_dim = latent_space_dim \n",
        "        self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.model = None\n",
        "\n",
        "        self._num_conv_layers = len(conv_filters)\n",
        "        self._shape_before_bottleneck = None\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.0001):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss=self._calculate_combined_loss,\n",
        "                           metrics=[self._calculate_reconstruction_loss,\n",
        "                                    self._calculate_kl_loss])\n",
        "\n",
        "    def train(self, x_train, batch_size, num_epochs):\n",
        "        self.model.fit(x_train,\n",
        "                       x_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       shuffle=True)\n",
        "\n",
        "    def save(self, save_folder=\".\"):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        latent_representations = self.encoder.predict(images)\n",
        "        reconstructed_images = self.decoder.predict(latent_representations)\n",
        "        return reconstructed_images, latent_representations\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "            self.conv_strides,\n",
        "            self.latent_space_dim\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            print(parameters)\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, save_folder=\".\"):\n",
        "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        autoencoder = VAE(*parameters)\n",
        "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        autoencoder.load_weights(weights_path)\n",
        "        return autoencoder\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                         + kl_loss\n",
        "        return combined_loss\n",
        "\n",
        "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "        error = y_target - y_predicted\n",
        "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                               K.exp(self.log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "        self._build_autoencoder()\n",
        "\n",
        "    def _build_autoencoder(self):\n",
        "        model_input = self._model_input\n",
        "        model_output = self.decoder(self.encoder(model_input))\n",
        "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        decoder_input = self._add_decoder_input()\n",
        "        dense_layer = self._add_dense_layer(decoder_input)\n",
        "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "    def _add_decoder_input(self):\n",
        "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "    def _add_dense_layer(self, decoder_input):\n",
        "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
        "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "        return dense_layer\n",
        "\n",
        "    def _add_reshape_layer(self, dense_layer):\n",
        "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "    def _add_conv_transpose_layers(self, x):\n",
        "        \"\"\"Add conv transpose blocks.\"\"\"\n",
        "        # loop through all the conv layers in reverse order and stop at the\n",
        "        # first layer\n",
        "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "            x = self._add_conv_transpose_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_transpose_layer(self, layer_index, x):\n",
        "        layer_num = self._num_conv_layers - layer_index\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_decoder_output(self, x):\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=self.conv_kernels[0],\n",
        "            strides=self.conv_strides[0],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "        return output_layer\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        encoder_input = self._add_encoder_input()\n",
        "        conv_layers = self._add_conv_layers(encoder_input)\n",
        "        bottleneck = self._add_bottleneck(conv_layers)\n",
        "        self._model_input = encoder_input\n",
        "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "    def _add_encoder_input(self):\n",
        "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "    def _add_conv_layers(self, encoder_input):\n",
        "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
        "        x = encoder_input\n",
        "        for layer_index in range(self._num_conv_layers):\n",
        "            x = self._add_conv_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_layer(self, layer_index, x):\n",
        "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
        "        conv 2d + ReLU + batch normalization.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        conv_layer = Conv2D(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"encoder_conv_layer_{layer_number}\"\n",
        "        )\n",
        "        x = conv_layer(x)\n",
        "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_bottleneck(self, x):\n",
        "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
        "        layer).\n",
        "        \"\"\"\n",
        "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "        x = Flatten()(x)\n",
        "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "        self.log_variance = Dense(self.latent_space_dim,\n",
        "                                  name=\"log_variance\")(x)\n",
        "\n",
        "        def sample_point_from_normal_distribution(args):\n",
        "            mu, log_variance = args\n",
        "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                      stddev=1.)\n",
        "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "            return sampled_point\n",
        "\n",
        "        x = Lambda(sample_point_from_normal_distribution,\n",
        "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(32, 64, 64, 64),\n",
        "        conv_kernels=(3, 3, 3, 3),\n",
        "        conv_strides=(1, 2, 2, 1),\n",
        "        latent_space_dim=32\n",
        "    )\n",
        "    # autoencoder.summary()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MetCvcrpWJ9g"
      },
      "source": [
        "2. Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4XDb66wU0qt",
        "outputId": "0ede61b2-e348-4e24-ca9e-0d2c030ad548"
      },
      "source": [
        "# from autoencoder import VAE\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 100\n",
        "LATENT_SPACE_DIM= 8 # try less than 10?\n",
        "\n",
        "SPECTROGRAMS_PATH = \"/content/drive/MyDrive/Music/VAE-test/spectrogram-01\"\n",
        "# FILE_NAME_REGEX = \"^512.*\"\n",
        "\n",
        "def load_fsdd(spectrograms_path):\n",
        "    dataset = {}\n",
        "    for root, _, file_names in os.walk(spectrograms_path):\n",
        "        for file_name in file_names:\n",
        "\n",
        "            if True: # re.match(regex, file_name):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                spectrogram = np.load(file_path) # (n_bins, n_frames, 1) \n",
        "                dataset[file_name] = spectrogram[..., np.newaxis]\n",
        "    return dataset\n",
        "\n",
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(256, 64, 1),\n",
        "        conv_filters=(32, 16, 16, 8), # 16, 8, 8, 4 -> \n",
        "        conv_kernels=(3, 3, 3, 3),\n",
        "        conv_strides=(1, 2, 2, 1),\n",
        "        latent_space_dim=LATENT_SPACE_DIM\n",
        "    )\n",
        "    autoencoder.summary()\n",
        "    autoencoder.compile(learning_rate)\n",
        "    autoencoder.train(x_train, batch_size, epochs)\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "dataset = load_fsdd(SPECTROGRAMS_PATH)\n",
        "\n",
        "autoencoder = train(np.array(list(dataset.values())), LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
        "model_suffix = \"-\" + str(LATENT_SPACE_DIM) + \"-\" + str(BATCH_SIZE) + \"-\" + str(EPOCHS)\n",
        "autoencoder.save(\"/content/drive/MyDrive/Music/VAE-test/model\" + model_suffix)\n",
        "print(\"Model Saved!!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 256, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv2D)   (None, 256, 64, 32)  320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_1 (ReLU)           (None, 256, 64, 32)  0           encoder_conv_layer_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_1 (BatchNormalizatio (None, 256, 64, 32)  128         encoder_relu_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv2D)   (None, 128, 32, 16)  4624        encoder_bn_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_2 (ReLU)           (None, 128, 32, 16)  0           encoder_conv_layer_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_2 (BatchNormalizatio (None, 128, 32, 16)  64          encoder_relu_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv2D)   (None, 64, 16, 16)   2320        encoder_bn_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_3 (ReLU)           (None, 64, 16, 16)   0           encoder_conv_layer_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_3 (BatchNormalizatio (None, 64, 16, 16)   64          encoder_relu_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_4 (Conv2D)   (None, 64, 16, 8)    1160        encoder_bn_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_4 (ReLU)           (None, 64, 16, 8)    0           encoder_conv_layer_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_4 (BatchNormalizatio (None, 64, 16, 8)    32          encoder_relu_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_23 (Flatten)            (None, 8192)         0           encoder_bn_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 8)            65544       flatten_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "log_variance (Dense)            (None, 8)            65544       flatten_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 8)            0           mu[0][0]                         \n",
            "                                                                 log_variance[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 139,800\n",
            "Trainable params: 139,656\n",
            "Non-trainable params: 144\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "decoder_dense (Dense)        (None, 8192)              73728     \n",
            "_________________________________________________________________\n",
            "reshape_23 (Reshape)         (None, 64, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 64, 16, 8)         584       \n",
            "_________________________________________________________________\n",
            "decoder_relu_1 (ReLU)        (None, 64, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_1 (BatchNormaliza (None, 64, 16, 8)         32        \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 128, 32, 16)       1168      \n",
            "_________________________________________________________________\n",
            "decoder_relu_2 (ReLU)        (None, 128, 32, 16)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_2 (BatchNormaliza (None, 128, 32, 16)       64        \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 64, 16)       2320      \n",
            "_________________________________________________________________\n",
            "decoder_relu_3 (ReLU)        (None, 256, 64, 16)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_3 (BatchNormaliza (None, 256, 64, 16)       64        \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 64, 1)        145       \n",
            "_________________________________________________________________\n",
            "sigmoid_layer (Activation)   (None, 256, 64, 1)        0         \n",
            "=================================================================\n",
            "Total params: 78,105\n",
            "Trainable params: 78,025\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 256, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 8)                 139800    \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 256, 64, 1)        78105     \n",
            "=================================================================\n",
            "Total params: 217,905\n",
            "Trainable params: 217,681\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "Train on 80 samples\n",
            "Epoch 1/100\n",
            "80/80 [==============================] - 2s 27ms/sample - loss: 140260.1289 - _calculate_reconstruction_loss: 0.1402 - _calculate_kl_loss: 31.5292\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 139350.5293 - _calculate_reconstruction_loss: 0.1392 - _calculate_kl_loss: 145.1351\n",
            "Epoch 3/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 126486.2441 - _calculate_reconstruction_loss: 0.1259 - _calculate_kl_loss: 625.6420\n",
            "Epoch 4/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 127640.5234 - _calculate_reconstruction_loss: 0.1258 - _calculate_kl_loss: 1800.9974\n",
            "Epoch 5/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 129895.2754 - _calculate_reconstruction_loss: 0.1277 - _calculate_kl_loss: 2180.1750\n",
            "Epoch 6/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 120679.2715 - _calculate_reconstruction_loss: 0.1182 - _calculate_kl_loss: 2511.6670\n",
            "Epoch 7/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 107249.6777 - _calculate_reconstruction_loss: 0.1059 - _calculate_kl_loss: 1378.4283\n",
            "Epoch 8/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 114968.8438 - _calculate_reconstruction_loss: 0.1140 - _calculate_kl_loss: 994.7010\n",
            "Epoch 9/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 111524.7266 - _calculate_reconstruction_loss: 0.1108 - _calculate_kl_loss: 719.1017\n",
            "Epoch 10/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 111248.3594 - _calculate_reconstruction_loss: 0.1105 - _calculate_kl_loss: 745.6673\n",
            "Epoch 11/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 108625.5801 - _calculate_reconstruction_loss: 0.1079 - _calculate_kl_loss: 746.5765\n",
            "Epoch 12/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 106637.7891 - _calculate_reconstruction_loss: 0.1057 - _calculate_kl_loss: 943.7394\n",
            "Epoch 13/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 103527.4414 - _calculate_reconstruction_loss: 0.1026 - _calculate_kl_loss: 974.2147\n",
            "Epoch 14/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 99068.4863 - _calculate_reconstruction_loss: 0.0982 - _calculate_kl_loss: 845.9546\n",
            "Epoch 15/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 103243.8672 - _calculate_reconstruction_loss: 0.1025 - _calculate_kl_loss: 781.7633\n",
            "Epoch 16/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 101105.9492 - _calculate_reconstruction_loss: 0.1006 - _calculate_kl_loss: 550.9001\n",
            "Epoch 17/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 100604.5820 - _calculate_reconstruction_loss: 0.1001 - _calculate_kl_loss: 503.3987\n",
            "Epoch 18/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 105679.7617 - _calculate_reconstruction_loss: 0.1052 - _calculate_kl_loss: 465.3988\n",
            "Epoch 19/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 104369.6328 - _calculate_reconstruction_loss: 0.1037 - _calculate_kl_loss: 670.0979\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 98105.0000 - _calculate_reconstruction_loss: 0.0970 - _calculate_kl_loss: 1071.4733\n",
            "Epoch 21/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 97456.8789 - _calculate_reconstruction_loss: 0.0967 - _calculate_kl_loss: 707.9056\n",
            "Epoch 22/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 101929.2520 - _calculate_reconstruction_loss: 0.1011 - _calculate_kl_loss: 829.0391\n",
            "Epoch 23/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 97139.7188 - _calculate_reconstruction_loss: 0.0956 - _calculate_kl_loss: 1562.2770\n",
            "Epoch 24/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 94965.5840 - _calculate_reconstruction_loss: 0.0939 - _calculate_kl_loss: 1092.8052\n",
            "Epoch 25/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 92084.7715 - _calculate_reconstruction_loss: 0.0914 - _calculate_kl_loss: 708.5396\n",
            "Epoch 26/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 93168.7246 - _calculate_reconstruction_loss: 0.0927 - _calculate_kl_loss: 470.9652\n",
            "Epoch 27/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 88674.1777 - _calculate_reconstruction_loss: 0.0884 - _calculate_kl_loss: 298.3304\n",
            "Epoch 28/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 86556.9238 - _calculate_reconstruction_loss: 0.0863 - _calculate_kl_loss: 281.1268\n",
            "Epoch 29/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 81077.1855 - _calculate_reconstruction_loss: 0.0809 - _calculate_kl_loss: 155.8778\n",
            "Epoch 30/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 78346.5391 - _calculate_reconstruction_loss: 0.0783 - _calculate_kl_loss: 84.3612\n",
            "Epoch 31/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 72831.2988 - _calculate_reconstruction_loss: 0.0728 - _calculate_kl_loss: 66.1279\n",
            "Epoch 32/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 65443.5879 - _calculate_reconstruction_loss: 0.0654 - _calculate_kl_loss: 68.3625\n",
            "Epoch 33/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 58981.0488 - _calculate_reconstruction_loss: 0.0589 - _calculate_kl_loss: 72.9696\n",
            "Epoch 34/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 54273.2109 - _calculate_reconstruction_loss: 0.0542 - _calculate_kl_loss: 69.3848\n",
            "Epoch 35/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 49359.3340 - _calculate_reconstruction_loss: 0.0493 - _calculate_kl_loss: 69.2414\n",
            "Epoch 36/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 44313.0244 - _calculate_reconstruction_loss: 0.0442 - _calculate_kl_loss: 68.0672\n",
            "Epoch 37/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 40398.0898 - _calculate_reconstruction_loss: 0.0403 - _calculate_kl_loss: 67.1238\n",
            "Epoch 38/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 37010.8164 - _calculate_reconstruction_loss: 0.0369 - _calculate_kl_loss: 65.1900\n",
            "Epoch 39/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 33858.0576 - _calculate_reconstruction_loss: 0.0338 - _calculate_kl_loss: 66.8713\n",
            "Epoch 40/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 32292.4126 - _calculate_reconstruction_loss: 0.0322 - _calculate_kl_loss: 68.2773\n",
            "Epoch 41/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 29236.2476 - _calculate_reconstruction_loss: 0.0292 - _calculate_kl_loss: 71.1771\n",
            "Epoch 42/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 27889.7402 - _calculate_reconstruction_loss: 0.0278 - _calculate_kl_loss: 74.3542\n",
            "Epoch 43/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 26771.5884 - _calculate_reconstruction_loss: 0.0267 - _calculate_kl_loss: 76.9617\n",
            "Epoch 44/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 25796.8950 - _calculate_reconstruction_loss: 0.0257 - _calculate_kl_loss: 79.2951\n",
            "Epoch 45/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 24663.9839 - _calculate_reconstruction_loss: 0.0246 - _calculate_kl_loss: 81.1516\n",
            "Epoch 46/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 24807.3569 - _calculate_reconstruction_loss: 0.0247 - _calculate_kl_loss: 83.0074\n",
            "Epoch 47/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 23478.4453 - _calculate_reconstruction_loss: 0.0234 - _calculate_kl_loss: 84.5341\n",
            "Epoch 48/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 23235.7246 - _calculate_reconstruction_loss: 0.0232 - _calculate_kl_loss: 85.2779\n",
            "Epoch 49/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 22972.8003 - _calculate_reconstruction_loss: 0.0229 - _calculate_kl_loss: 85.9673\n",
            "Epoch 50/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 21783.2363 - _calculate_reconstruction_loss: 0.0217 - _calculate_kl_loss: 87.5482\n",
            "Epoch 51/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 21290.7778 - _calculate_reconstruction_loss: 0.0212 - _calculate_kl_loss: 88.3988\n",
            "Epoch 52/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 21377.2349 - _calculate_reconstruction_loss: 0.0213 - _calculate_kl_loss: 88.8171\n",
            "Epoch 53/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 20801.7026 - _calculate_reconstruction_loss: 0.0207 - _calculate_kl_loss: 88.1672\n",
            "Epoch 54/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 21288.7344 - _calculate_reconstruction_loss: 0.0212 - _calculate_kl_loss: 87.7717\n",
            "Epoch 55/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 20067.2427 - _calculate_reconstruction_loss: 0.0200 - _calculate_kl_loss: 87.4650\n",
            "Epoch 56/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 19773.6963 - _calculate_reconstruction_loss: 0.0197 - _calculate_kl_loss: 87.6666\n",
            "Epoch 57/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 19751.6646 - _calculate_reconstruction_loss: 0.0197 - _calculate_kl_loss: 87.2938\n",
            "Epoch 58/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 19338.6816 - _calculate_reconstruction_loss: 0.0193 - _calculate_kl_loss: 86.8086\n",
            "Epoch 59/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 19063.3472 - _calculate_reconstruction_loss: 0.0190 - _calculate_kl_loss: 86.4795\n",
            "Epoch 60/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 18783.5225 - _calculate_reconstruction_loss: 0.0187 - _calculate_kl_loss: 86.1542\n",
            "Epoch 61/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 18694.0337 - _calculate_reconstruction_loss: 0.0186 - _calculate_kl_loss: 85.6716\n",
            "Epoch 62/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 18268.0874 - _calculate_reconstruction_loss: 0.0182 - _calculate_kl_loss: 85.5025\n",
            "Epoch 63/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 18488.8730 - _calculate_reconstruction_loss: 0.0184 - _calculate_kl_loss: 85.2444\n",
            "Epoch 64/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 18093.5288 - _calculate_reconstruction_loss: 0.0180 - _calculate_kl_loss: 85.3145\n",
            "Epoch 65/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 17561.0654 - _calculate_reconstruction_loss: 0.0175 - _calculate_kl_loss: 85.0997\n",
            "Epoch 66/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 17531.2383 - _calculate_reconstruction_loss: 0.0174 - _calculate_kl_loss: 84.9784\n",
            "Epoch 67/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 17504.2520 - _calculate_reconstruction_loss: 0.0174 - _calculate_kl_loss: 84.4223\n",
            "Epoch 68/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 17219.2937 - _calculate_reconstruction_loss: 0.0171 - _calculate_kl_loss: 83.6300\n",
            "Epoch 69/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16921.4800 - _calculate_reconstruction_loss: 0.0168 - _calculate_kl_loss: 83.0752\n",
            "Epoch 70/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16842.0237 - _calculate_reconstruction_loss: 0.0168 - _calculate_kl_loss: 83.1569\n",
            "Epoch 71/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16573.9788 - _calculate_reconstruction_loss: 0.0165 - _calculate_kl_loss: 83.3319\n",
            "Epoch 72/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16598.2808 - _calculate_reconstruction_loss: 0.0165 - _calculate_kl_loss: 82.8197\n",
            "Epoch 73/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 17206.3423 - _calculate_reconstruction_loss: 0.0171 - _calculate_kl_loss: 82.5677\n",
            "Epoch 74/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16502.0195 - _calculate_reconstruction_loss: 0.0164 - _calculate_kl_loss: 82.6491\n",
            "Epoch 75/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16193.8928 - _calculate_reconstruction_loss: 0.0161 - _calculate_kl_loss: 82.7133\n",
            "Epoch 76/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 16182.2021 - _calculate_reconstruction_loss: 0.0161 - _calculate_kl_loss: 83.2871\n",
            "Epoch 77/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 16411.4382 - _calculate_reconstruction_loss: 0.0163 - _calculate_kl_loss: 83.1296\n",
            "Epoch 78/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 15544.4426 - _calculate_reconstruction_loss: 0.0155 - _calculate_kl_loss: 82.9028\n",
            "Epoch 79/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 15864.7617 - _calculate_reconstruction_loss: 0.0158 - _calculate_kl_loss: 83.5891\n",
            "Epoch 80/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 15359.4478 - _calculate_reconstruction_loss: 0.0153 - _calculate_kl_loss: 83.4139\n",
            "Epoch 81/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 15385.7068 - _calculate_reconstruction_loss: 0.0153 - _calculate_kl_loss: 83.4076\n",
            "Epoch 82/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14960.2886 - _calculate_reconstruction_loss: 0.0149 - _calculate_kl_loss: 83.3461\n",
            "Epoch 83/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 14915.0540 - _calculate_reconstruction_loss: 0.0148 - _calculate_kl_loss: 83.2655\n",
            "Epoch 84/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14707.1553 - _calculate_reconstruction_loss: 0.0146 - _calculate_kl_loss: 82.8707\n",
            "Epoch 85/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14941.7915 - _calculate_reconstruction_loss: 0.0149 - _calculate_kl_loss: 82.5889\n",
            "Epoch 86/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 15132.2607 - _calculate_reconstruction_loss: 0.0150 - _calculate_kl_loss: 82.5917\n",
            "Epoch 87/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14585.7480 - _calculate_reconstruction_loss: 0.0145 - _calculate_kl_loss: 82.6644\n",
            "Epoch 88/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14953.0696 - _calculate_reconstruction_loss: 0.0149 - _calculate_kl_loss: 83.2337\n",
            "Epoch 89/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14288.9490 - _calculate_reconstruction_loss: 0.0142 - _calculate_kl_loss: 82.9283\n",
            "Epoch 90/100\n",
            "80/80 [==============================] - 0s 4ms/sample - loss: 14154.8538 - _calculate_reconstruction_loss: 0.0141 - _calculate_kl_loss: 82.6241\n",
            "Epoch 91/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14353.8337 - _calculate_reconstruction_loss: 0.0143 - _calculate_kl_loss: 82.7237\n",
            "Epoch 92/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14085.0222 - _calculate_reconstruction_loss: 0.0140 - _calculate_kl_loss: 83.0284\n",
            "Epoch 93/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13924.5825 - _calculate_reconstruction_loss: 0.0138 - _calculate_kl_loss: 82.9590\n",
            "Epoch 94/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14079.9749 - _calculate_reconstruction_loss: 0.0140 - _calculate_kl_loss: 83.1870\n",
            "Epoch 95/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13736.2268 - _calculate_reconstruction_loss: 0.0137 - _calculate_kl_loss: 82.8952\n",
            "Epoch 96/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 14155.9294 - _calculate_reconstruction_loss: 0.0141 - _calculate_kl_loss: 82.6672\n",
            "Epoch 97/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13736.8574 - _calculate_reconstruction_loss: 0.0137 - _calculate_kl_loss: 82.4581\n",
            "Epoch 98/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13805.2861 - _calculate_reconstruction_loss: 0.0137 - _calculate_kl_loss: 82.6707\n",
            "Epoch 99/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13524.5686 - _calculate_reconstruction_loss: 0.0134 - _calculate_kl_loss: 83.1699\n",
            "Epoch 100/100\n",
            "80/80 [==============================] - 0s 5ms/sample - loss: 13476.2942 - _calculate_reconstruction_loss: 0.0134 - _calculate_kl_loss: 83.1172\n",
            "[(256, 64, 1), (32, 16, 16, 8), (3, 3, 3, 3), (1, 2, 2, 1), 8]\n",
            "Model Saved!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2IhMH3guEs_",
        "outputId": "60e86104-d978-49d8-9656-e02803b53d4d"
      },
      "source": [
        "# # https://github.com/musikalkemist/generating-sound-with-neural-networks/blob/49d7db32c43d1a04c596cbbb282a9521be1e7fc8/11%20Implementing%20VAE/code/analysis.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "LATENT_REPRESENTATIONS_PATH = '/content/drive/MyDrive/Music/VAE-test/model-' + str(LATENT_SPACE_DIM) + '-' + str(BATCH_SIZE) + '-' + str(EPOCHS) + '/'\n",
        "MODEL_PATH = '/content/drive/MyDrive/Music/VAE-test/model-8-20-100'\n",
        "\n",
        "def select_random_images(dataset, num_images=10):\n",
        "    sample_keys = np.random.choice(list(dataset.keys()), num_images)\n",
        "    sample_ds = { key: dataset[key] for key in sample_keys }\n",
        "\n",
        "    return sample_ds\n",
        "\n",
        "\n",
        "def plot_reconstructed_images(images, reconstructed_images):\n",
        "    num_images = len(images)\n",
        "    for i, (image, reconstructed_image) in enumerate(zip(images, reconstructed_images)):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        image = image.squeeze()\n",
        "        img = librosa.display.specshow(image, y_axis='log', x_axis='time', ax=ax)\n",
        "        fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        reconstructed_image = reconstructed_image.squeeze()\n",
        "        recon_img = librosa.display.specshow(reconstructed_image, y_axis='log', x_axis='time', ax=ax)\n",
        "        fig.colorbar(recon_img, ax=ax, format=\"%+2.0f dB\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_images_encoded_in_latent_space(latent_representations, sample_labels):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(latent_representations[:, 0],\n",
        "                latent_representations[:, 1],\n",
        "                cmap=\"rainbow\",\n",
        "                c=sample_labels,\n",
        "                alpha=0.5,\n",
        "                s=2)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "def download_vectors(autoencoder, sample_ds, download_path):\n",
        "    images = np.array(list(sample_ds.values()))\n",
        "    filenames = sample_ds.keys()\n",
        "    latent_representations = autoencoder.encoder.predict(images)\n",
        "\n",
        "    with open(download_path + 'embeddings.tsv', 'w', newline='') as f_output:\n",
        "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
        "        tsv_output.writerows(latent_representations)\n",
        "    # Write corresdonding filenames\n",
        "    with open(download_path + 'embedding-filenames.tsv', 'a') as f_output:\n",
        "        f_output.seek(0)\n",
        "        f_output.truncate()\n",
        "        for data in filenames:\n",
        "            f_output.write(data)\n",
        "            f_output.write('\\n')\n",
        "    return latent_representations, filenames\n",
        "\n",
        "\n",
        "autoencoder = VAE.load(MODEL_PATH)\n",
        "dataset = load_fsdd(SPECTROGRAMS_PATH)\n",
        "\n",
        "num_sample_images_to_show = 30\n",
        "\n",
        "sample_ds = select_random_images(dataset, num_sample_images_to_show)\n",
        "reconstructed_images, _ = autoencoder.reconstruct(np.array(list(sample_ds.values())))\n",
        "download_vectors(autoencoder, sample_ds, LATENT_REPRESENTATIONS_PATH)\n",
        "\n",
        "# plot_reconstructed_images(sample_images, reconstructed_images)\n",
        "\n",
        "# num_images = 6000\n",
        "# sample_images, sample_labels = select_images(x_test, y_test, num_images)\n",
        "# _, latent_representations = autoencoder.reconstruct(sample_images)\n",
        "# plot_images_encoded_in_latent_space(latent_representations, sample_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.9309763 ,  0.7092061 , -2.96149   , -2.3542452 ,  2.639731  ,\n",
              "          1.6745735 ,  1.2930074 , -2.6118832 ],\n",
              "        [ 5.0017257 ,  1.8820826 , -1.0214549 , -2.5994651 ,  4.3479624 ,\n",
              "          4.225523  ,  6.6070724 , -5.3922234 ],\n",
              "        [ 3.0887406 ,  1.2084757 , -2.8370576 , -3.0406609 ,  2.996209  ,\n",
              "          2.1580844 ,  2.2900174 , -3.5587275 ],\n",
              "        [ 4.402467  ,  1.6606225 , -2.5379202 , -3.3595417 ,  3.883297  ,\n",
              "          3.3562305 ,  4.240099  , -4.7738056 ],\n",
              "        [ 4.4007053 ,  1.6625434 , -1.8928865 , -2.8953092 ,  3.933433  ,\n",
              "          3.6980517 ,  4.9839478 , -4.973613  ],\n",
              "        [ 3.331373  ,  1.2960435 , -2.7855191 , -3.0281904 ,  3.3010225 ,\n",
              "          2.4388483 ,  2.5970416 , -3.7551234 ],\n",
              "        [ 5.113605  ,  2.145961  , -2.6577506 , -3.630461  ,  4.5887175 ,\n",
              "          4.1634016 ,  5.4116945 , -5.602186  ],\n",
              "        [ 4.111113  ,  1.7056317 , -1.5980251 , -2.5654817 ,  3.7988243 ,\n",
              "          3.671735  ,  4.601552  , -4.5234203 ],\n",
              "        [ 4.274681  ,  1.7730095 , -2.4832091 , -3.2291017 ,  3.846049  ,\n",
              "          3.6524754 ,  4.395424  , -4.819671  ],\n",
              "        [ 3.0054471 ,  1.3786929 , -2.5534258 , -2.2880764 ,  2.914149  ,\n",
              "          2.5412822 ,  2.6584275 , -3.4732633 ],\n",
              "        [ 5.549475  ,  2.0512547 , -1.7429845 , -3.367885  ,  4.913199  ,\n",
              "          4.5623655 ,  6.466876  , -5.7738404 ],\n",
              "        [ 4.0117846 ,  1.6337196 , -2.703356  , -3.3334653 ,  3.7415478 ,\n",
              "          3.198616  ,  3.5839825 , -4.524866  ],\n",
              "        [ 4.4012475 ,  1.915571  , -2.2336047 , -3.3523157 ,  3.86655   ,\n",
              "          3.5948563 ,  4.413918  , -4.737682  ],\n",
              "        [ 6.4810615 ,  2.5764122 , -1.5027198 , -3.6266015 ,  5.525674  ,\n",
              "          5.3160005 ,  7.6615024 , -6.4787016 ],\n",
              "        [ 2.4165547 ,  0.6073702 , -2.8283777 , -2.3505218 ,  2.8430963 ,\n",
              "          1.9373872 ,  1.7415754 , -3.059272  ],\n",
              "        [ 3.648579  ,  0.87306035, -1.3101782 , -1.4863373 ,  3.018716  ,\n",
              "          2.5118508 ,  3.8842337 , -4.396171  ],\n",
              "        [ 4.819175  ,  1.8709284 , -1.0757326 , -2.8273737 ,  4.2938204 ,\n",
              "          3.9589133 ,  5.9084773 , -4.9208302 ],\n",
              "        [ 4.610641  ,  1.6942426 , -1.7824911 , -2.8450203 ,  3.8558686 ,\n",
              "          3.667767  ,  5.191731  , -5.1426463 ],\n",
              "        [ 5.40261   ,  2.2247589 , -2.3258522 , -3.766648  ,  4.5968523 ,\n",
              "          4.553974  ,  5.899984  , -5.8590117 ],\n",
              "        [ 5.2884097 ,  2.2869337 , -1.9302003 , -3.193448  ,  4.555234  ,\n",
              "          4.4438334 ,  6.154057  , -5.6830115 ],\n",
              "        [ 4.7991686 ,  2.0475788 , -2.0684328 , -3.3065035 ,  4.237725  ,\n",
              "          3.7765496 ,  5.115858  , -5.3675985 ],\n",
              "        [ 4.4503646 ,  1.9398854 , -2.282378  , -3.2982447 ,  4.204341  ,\n",
              "          3.7143145 ,  4.59804   , -5.092436  ],\n",
              "        [ 3.217095  ,  1.4278886 , -2.493798  , -3.1489062 ,  3.3079057 ,\n",
              "          2.5120246 ,  2.463906  , -3.764319  ],\n",
              "        [ 4.860412  ,  2.0784783 , -2.5446112 , -3.2875583 ,  4.3596687 ,\n",
              "          4.0083365 ,  5.1867223 , -5.41053   ]], dtype=float32),\n",
              " dict_keys(['00034-01 09B BAS_SVT-4 Pro.wav.npy', '00049-01 13A Cowboys from DFW.wav.npy', '00009-01 03A Archetype Clean.wav.npy', '00021-01 06A Cali Texas Ch 1.wav.npy', '00010-01 03B Matchstick Ch1.wav.npy', '00064-01 16D FELIX DELUXE MOD.wav.npy', '00026-01 07B Line 6 Litigator.wav.npy', '00029-01 08A US Small Tweed.wav.npy', '00005-01 02A US Deluxe Nrm.wav.npy', '00035-01 09C BAS_Cali Bass.wav.npy', '00008-01 02D Revv Gen Red.wav.npy', '00017-01 05A Placater Clean.wav.npy', '00054-01 14B DUSTED.wav.npy', '00079-01 20C BILLY KASTODON.wav.npy', '00033-01 09A DI.wav.npy', '00067-01 17C BUMBLE ACOUSTIC.wav.npy', '00012-01 03D Archetype Lead.wav.npy', '00024-01 06D German Mahadeva.wav.npy', '00076-01 19D EMPTY GARBAGE.wav.npy', '00015-01 04C Brit J45 Brt.wav.npy', '00052-01 13D BIG VENUE DRIVE.wav.npy', '00022-01 06B Essex A15.wav.npy', '00036-01 09D BAS_Aqua 51.wav.npy', '00014-01 04B Fullerton Brt.wav.npy']))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}